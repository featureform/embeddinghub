name: Testing
on:
  push:

concurrency:
  group: ${{ github.head_ref }}-testing
  cancel-in-progress: true

env:
  MEILISEARCH_PORT: 7700
  MEILISEARCH_API_KEY: ""
  REDISEARCH_INSECURE_PORT: 6380
  REDIS_INSECURE_PORT: 6379
  REDIS_SECURE_PORT: 6378
  REDIS_PASSWORD: "password"
  CASSANDRA_USER: "cassandra"
  CASSANDRA_PASSWORD: "CASSANDRA"
  POSTGRES_USER: "username"
  POSTGRES_DB: "default"
  POSTGRES_PASSWORD: "password"
  ETCD_HOST: "localhost"
  ETCD_PORT: 2379
  REDSHIFT_PORT: 5439
  REDSHIFT_DATABASE: dev
  SPARK_LOCAL_SCRIPT_PATH: scripts/spark/offline_store_spark_runner.py
  PYTHON_LOCAL_INIT_PATH: scripts/spark/python_packages.sh
  PINECONE_PROJECT_ID: ${{ secrets.PINECONE_PROJECT_ID }}
  PINECONE_ENVIRONMENT: ${{ secrets.PINECONE_ENVIRONMENT }}
  PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}

jobs:
  go-tests:
    name: Run Go Tests
    defaults:
      run:
        working-directory: ./
    runs-on: ubuntu-latest
    timeout-minutes: 120
    environment: Integration testing
    services:
      redis-insecure:
        image: redis
        # Hard coded port because environment variables not currently
        # supported for use outside of 'steps'
        ports:
          - 6379:6379

      cassandra:
        image: cassandra
        # Hard coded port because environment variables not currently
        # supported for use outside of 'steps'
        ports:
          - 9042:9042

      redisearch:
        image: redis/redis-stack
        # Hard coded port because environment variables not currently
        # supported for use outside of 'steps'
        ports:
          - 6380:6379
      postgres:
        image: postgres
        ports:
          - 5432:5432
        env:
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
    steps:
      - uses: actions/checkout@v2

      - name: Check directory
        run: |
          ls -la

      - name: Set up Go
        uses: actions/setup-go@v2
        with:
          go-version: 1.21

      - name: Install grpc_tools
        run: pip install grpcio-tools build

      - name: Install pyspark packages
        run: |
          pip install -r provider/scripts/k8s/requirements.txt
          pip install -r provider/scripts/spark/requirements.txt

      - name: Install Protobuf
        run: sudo snap install protobuf --classic

      - name: Setup Proto
        run: ./gen_grpc.sh

      - name: Unit Tests
        run: go test ./... -short

      - name: Install Search Container
        run: docker pull getmeili/meilisearch:v1.0

      - name: Start Search
        run: |
          docker run -d -p $MEILISEARCH_PORT:7700 getmeili/meilisearch:v1.0

      - uses: getong/redis-action@v1
        with:
          host port: 6378
          container port: 6379
          redis password: "password"

      - name: create-json
        id: create-json
        uses: jsdaniell/create-json@1.1.2
        with:
          name: "./provider/firestore_credentials.json"
          json: ${{ secrets.FIRESTORE_CREDENTIALS_FILE }}

      - name: create-json
        id: create-json-2
        uses: jsdaniell/create-json@1.1.2
        with:
          name: "./provider/bigquery_credentials.json"
          json: ${{ secrets.BIGQUERY_CREDENTIALS_FILE }}

      - name: Install ETCD
        run: |
          git clone -b v3.4.16 https://github.com/etcd-io/etcd.git
          cd etcd
          ./build
          export PATH="$PATH:`pwd`/bin"
          etcd --version
          etcd --logger=zap &

      - name: Integration Tests
        run: go test ./... -parallel 1000
        env:
          DYNAMO_ACCESS_KEY: ${{ secrets.AWS_ACCESS_KEY_ID }}
          DYNAMO_SECRET_KEY: ${{ secrets.AWS_SECRET_KEY }}
          DYNAMODB_REGION: "us-east-1"
          FIRESTORE_CRED: "firestore_credentials.json"
          FIRESTORE_PROJECT: ${{ secrets.FIRESTORE_PROJECT }}
          AZURE_ACCOUNT_NAME: ${{ secrets.AZURE_ACCOUNT_NAME }}
          AZURE_ACCOUNT_KEY: ${{ secrets.AZURE_ACCOUNT_KEY }}
          AZURE_CONTAINER_NAME: ${{ secrets.AZURE_CONTAINER_NAME }}
          AZURE_BACKUP_STORAGE_PATH: "backup"
          MONGODB_HOST: ${{ secrets.MONGODB_HOST }}
          MONGODB_PORT: ${{ secrets.MONGODB_PORT }}
          MONGODB_USERNAME: ${{ secrets.MONGODB_USERNAME }}
          MONGODB_PASSWORD: ${{ secrets.MONGODB_PASSWORD }}
          MONGODB_DATABASE: ${{ secrets.MONGODB_DATABASE }}
          SNOWFLAKE_USERNAME: ${{ secrets.SNOWFLAKE_USERNAME }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_ORG: ${{ secrets.SNOWFLAKE_ORG }}
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          REDSHIFT_USERNAME: ${{ secrets.REDSHIFT_USERNAME }}
          REDSHIFT_PASSWORD: ${{ secrets.REDSHIFT_PASSWORD }}
          REDSHIFT_ENDPOINT: ${{ secrets.REDSHIFT_ENDPOINT }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_KEY: ${{ secrets.AWS_SECRET_KEY }}
          AWS_EMR_CLUSTER_REGION: ${{ secrets.AWS_EMR_CLUSTER_REGION }}
          AWS_EMR_CLUSTER_ID: ${{ secrets.AWS_EMR_CLUSTER_ID }}
          S3_BUCKET_PATH: ${{ secrets.S3_BUCKET_PATH }}
          S3_BUCKET_REGION: ${{ secrets.S3_BUCKET_REGION }}
          BIGQUERY_PROJECT_ID: ${{ secrets.BIGQUERY_PROJECT_ID }}
          BIGQUERY_DATASET_ID: ${{ secrets.BIGQUERY_DATASET_ID }}
          BIGQUERY_CREDENTIALS: "provider/bigquery_credentials.json"
          MYSQL_USER: "root"
          MYSQL_PASSWORD: "password"
          MYSQL_DB: "mysql"
          AZURE_CONTAINER_PATH: ${{ secrets.AZURE_CONTAINER_PATH }}
          AZURE_CONNECTION_STRING: ${{ secrets.AZURE_CONNECTION_STRING }}
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          DATABRICKS_CLUSTER: ${{ secrets.DATABRICKS_CLUSTER }}
          MATERIALIZE_NO_TIMESTAMP_QUERY_PATH: /home/runner/work/featureform/featureform/provider/queries/materialize_no_ts.sql
          MATERIALIZE_WITH_TIMESTAMP_QUERY_PATH: /home/runner/work/featureform/featureform/provider/queries/materialize_ts.sql
          ETCD_HOST: ${{ env.ETCD_HOST }}
          ETCD_PORT: ${{ env.ETCD_PORT }}
          SPARK_LOCAL_SCRIPT_PATH: home/runner/work/featureform/featureform/provider/scripts/spark/offline_store_spark_runner.py
          PYTHON_LOCAL_INIT_PATH: home/runner/work/featureform/featureform/provider/scripts/spark/python_packages.sh



#  backup:
#    name: Backup Test
#    needs: setup
#    runs-on: ubuntu-latest
#
#    steps:
#      - name: Download Working Compiled Directories
#        uses: actions/download-artifact@v3
#        with:
#          name: compiled-workdir
#
#      - name: Set up Go
#        uses: actions/setup-go@v2
#        with:
#          go-version: 1.21
#
#      # Should be switched to a container, but the available etcd containers
#      # haven't been running easily locally yet. Will create a custom one
#      # in the future
#      - name: Install ETCD
#        run: |
#          git clone -b v3.4.16 https://github.com/etcd-io/etcd.git
#          cd etcd
#          ./build
#          echo "Adding etcd to github actions path:"
#          echo "`pwd`/bin"
#          echo "`pwd`/bin" >> $GITHUB_PATH
#
#      - name: Check Path
#        run: |
#          echo $GITHUB_PATH
#          etcd --version
#          etcd --logger=zap &
#
#      - name: Set Permissions
#        run: chmod +x ./tests/integration/backup/test.sh
#
#      - name: Run Backup Test Local
#        env:
#          CLOUD_PROVIDER: "LOCAL_FILESYSTEM"
#        run: ./tests/integration/backup/test.sh
#
#      - name: Run Backup Test Azure
#        env:
#          CLOUD_PROVIDER: "AZURE"
#          AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_ACCOUNT_NAME }}
#          AZURE_STORAGE_KEY: ${{ secrets.AZURE_ACCOUNT_KEY }}
#          AZURE_CONTAINER_NAME: ${{ secrets.AZURE_CONTAINER_NAME }}
#          AZURE_STORAGE_PATH: "backup"
#        run: ./tests/integration/backup/test.sh
#
#      - name: Run Backup Test S3
#        env:
#          CLOUD_PROVIDER: "S3"
#          AWS_ACCESS_KEY: ${{ secrets.AWS_ACCESS_KEY_ID }}
#          AWS_SECRET_KEY: ${{ secrets.AWS_SECRET_KEY }}
#          AWS_BUCKET_REGION: "us-east-1"
#          AWS_BUCKET_NAME: "featureform-testing"
#          AWS_BUCKET_PATH: "backup"
#        run: ./tests/integration/backup/test.sh
#
#      - name: Run Backup Test GCS
#        env:
#          CLOUD_PROVIDER: "GCS"
#          GCS_BUCKET_NAME: "featureform-test"
#          GCS_BUCKET_PATH: "backup"
#          GCS_CREDENTIALS: ${{ secrets.GCS_CREDENTIALS }}
#        run: ./tests/integration/backup/test.sh


  spark-script:
    name: PySpark Script Testing
    environment: Integration testing
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install python dependencies
        working-directory: ./provider/scripts/spark
        run: python3 -m pip install -r requirements.txt

      - name: Testing

        run: make test_pyspark


  localmode:
    name: Test Localmode
    runs-on: ${{ matrix.os }}
    if: github.ref != 'refs/heads/main'
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest] #osx-arm64 currently not working. Ticket made with github
        python-version: ["3.7", "3.8", "3.9", "3.10", "3.11"]
        exclude:
          - os: windows-latest
            python-version: "3.11"
    steps:
      - uses: actions/checkout@v2

      #
      # https://github.com/actions/setup-python/issues/682
      # Python 3.7.17 in MacOSX will raise "No module named _bz2" exception, so pinning to 3.7.16
      #
      # Previous setup, revert back to the below once th3 issue is fixed:
      # - name: Set up Python
      #   uses: actions/setup-python@v4
      #   with:
      #     python-version: ${{ matrix.python-version }}
      #
      - uses: actions/setup-python@v4
        if: matrix.os != 'macos-latest' || ( matrix.os == 'macos-latest' && matrix.python-version != '3.7' )
        with:
          python-version: ${{ matrix.python-version }}
      - uses: actions/setup-python@v4
        if: matrix.os == 'macos-latest' && matrix.python-version == '3.7'
        with:
          python-version: "3.7.16"

      - name: Install grpc_tools
        run: pip install grpcio-tools build

      - name: Install pyspark packages
        run: |
          pip install -r provider/scripts/k8s/requirements.txt
          pip install -r provider/scripts/spark/requirements.txt

      - name: Install Protobuf
        run: sudo snap install protobuf --classic

      - name: Setup Proto
        run: ./gen_grpc.sh

      - name: Setup Proto
        run: ./pip_update.sh --no-dash

      - name: Install pytest
        run: pip install pytest protobuf==3.20.1 python-dotenv pytest-mock pyspark

      - name: Install featureform
        run: pip install client/dist/featureform-0.0.0-py3-none-any.whl

      - name: Run Tests
        run: make pytest


